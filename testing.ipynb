{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Imports for testing purposes\n",
    "'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from ast import literal_eval\n",
    "from datetime import datetime\n",
    "from utils.base_templates import NewsArticle, ArticleCollection, Company, Insider\n",
    "\n",
    "import database.database_creator as dc\n",
    "import database.database_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"database/insiders_2024-10-22.csv\"\n",
    "output_file = \"database/full_insiders_info_2024-10-22.csv\"\n",
    "\n",
    "dc.save_insiders_to_csv(input_file, output_file)\n",
    "dc.scrape_all_companies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'isin', 'ticker', 'industry', 'sector', 'profile', 'executives',\n",
       "       'link', 'country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('database/companies_info_2024-11-01.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_company_and_insiders_from_article(article_link: str):\n",
    "    # Step 1: Fetch the article from the Articles collection\n",
    "    articles_collection = du.get_db_collection(du.DB_NAME, 'Articles')\n",
    "    article = articles_collection.find_one({\"link\": article_link})\n",
    "    \n",
    "    if not article:\n",
    "        article = dc.get_article_from_link(article_link)\n",
    "        if not article:\n",
    "            return None\n",
    "    # Step 2: Fetch the company linked to this article\n",
    "    company_link = 'https://www.marketscreener.com' + article.get('company_link', '')\n",
    "    companies_collection = du.get_db_collection(du.DB_NAME, 'Companies')\n",
    "    company = companies_collection.find_one({\"link\": company_link})\n",
    "    \n",
    "    if not company:\n",
    "        # If the company is not found, scrape it\n",
    "        ticker, isin, company_profile, executives = dc.extract_company_info(company_link)\n",
    "        if not ticker:\n",
    "            print(\"Company not found and could not be scraped.\")\n",
    "            return None\n",
    "        company = {\n",
    "            'link': company_link,\n",
    "            'ticker': ticker,\n",
    "            'isin': isin,\n",
    "            'profile': company_profile,\n",
    "            'executives': executives\n",
    "        }\n",
    "        # Save the scraped company data to the database\n",
    "        companies_collection.insert_one(company)\n",
    "\n",
    "    # Step 3: Get executive 'href' links to query the Insiders collection\n",
    "    executives = dc.str_to_dict_expansion(company.get('executives', '{}'))\n",
    "    insider_links = [\n",
    "        exec_info['href'] \n",
    "        for exec_category in executives.values() \n",
    "        for exec_info in exec_category.values()\n",
    "    ]\n",
    "\n",
    "    # Step 4: Fetch insiders based on the collected links\n",
    "    insiders_collection = du.get_db_collection(du.DB_NAME, 'Insiders')\n",
    "    insiders = list(insiders_collection.find({\"link\": {\"$in\": insider_links}}))\n",
    "\n",
    "    if not insiders:\n",
    "        # If insiders are not found, scrape them\n",
    "        for link in insider_links:\n",
    "            # Retrieve the name from the collection\n",
    "            exec_info = next(\n",
    "                (info for exec_category in executives.values() for info in exec_category.values() if info['href'] == link),\n",
    "                None\n",
    "            )\n",
    "            name = exec_info.get('name') if exec_info else 'Unknown'\n",
    "\n",
    "            # Pass the name as an additional argument\n",
    "            insider = dc.extract_insider_info(name, link)  # Assume this function is defined in database_creator.py\n",
    "            if insider:\n",
    "                insiders.append(insider)\n",
    "                # Save the scraped insider data to the database\n",
    "                insiders_collection.insert_one(insider.dict())\n",
    "\n",
    "    # Return the collected data as a dictionary\n",
    "    return {\n",
    "        \"article\": article,\n",
    "        \"company\": company,\n",
    "        \"insiders\": insiders\n",
    "    }\n",
    "\n",
    "def generate_markdown_report(article_link: str, output_dir: str = \"reports\"):\n",
    "    # Fetch article, company, and insiders data\n",
    "    data = find_company_and_insiders_from_article(article_link)\n",
    "    if data is None:\n",
    "        return\n",
    "\n",
    "    article = data[\"article\"]\n",
    "    company = data[\"company\"]\n",
    "    insiders = data[\"insiders\"]\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Generate the markdown file name using the article headline and date\n",
    "    headline = article.get('headline', 'unknown_headline').replace(' ', '_')\n",
    "    publication_date = article.get('publication_date', 'unknown_date')\n",
    "    file_name = f\"{headline}_{publication_date}.md\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # Write the markdown report\n",
    "    with open(file_path, 'w') as md_file:\n",
    "        md_file.write(f\"# {article.get('headline')}\\n\")\n",
    "        md_file.write(f\"**Publication Date:** {article.get('publication_date')}\\n\")\n",
    "        md_file.write(f\"**Category:** {article.get('category')}\\n\")\n",
    "        md_file.write(f\"**Source:** {article.get('source')}\\n\\n\")\n",
    "        md_file.write(\"## Article Content\\n\")\n",
    "        md_file.write(f\"{article.get('content')}\\n\\n\")\n",
    "        md_file.write(\"## Company Information\\n\")\n",
    "        md_file.write(f\"- **Name:** {company.get('name')}\\n\")\n",
    "        md_file.write(f\"- **ISIN:** {company.get('isin')}\\n\")\n",
    "        md_file.write(f\"- **Ticker:** {company.get('ticker')}\\n\")\n",
    "        md_file.write(f\"- **Industry:** {company.get('industry')}\\n\")\n",
    "        md_file.write(f\"- **Sector:** {company.get('sector')}\\n\")\n",
    "        md_file.write(f\"- **Country:** {company.get('country')}\\n\")\n",
    "        profile = company.get('profile', '').replace('-', '\\\\-')\n",
    "        md_file.write(f\"- **Profile:**\\n {profile}\\n\\n\")\n",
    "        md_file.write(\"## Linked Insiders\\n\")\n",
    "        for insider in insiders:\n",
    "            md_file.write(f\"- **Name:** {insider.get('name')}\\n\")\n",
    "            md_file.write(f\"- **Current Position:** {insider.get('current_position')}\\n\")\n",
    "            md_file.write(f\"- **Current Company:** {insider.get('current_company')}\\n\")\n",
    "            md_file.write(f\"- **Company URL:** {insider.get('company_url')}\\n\")\n",
    "            md_file.write(f\"- **Net Worth:** {insider.get('net_worth', 'N/A')}\\n\")\n",
    "            known_holdings = dc.str_to_dict_expansion(insider.get('known_holdings', '{}'))\n",
    "            md_file.write(f\"- **Known Holdings:**\\n\")\n",
    "            for company, details in known_holdings.items():\n",
    "                md_file.write(f\"  - **{company}:**\\n\")\n",
    "                md_file.write(f\"    - **Link:** {details.get('link', 'N/A')}\\n\")\n",
    "                md_file.write(f\"    - **Date:** {details.get('date', 'N/A')}\\n\")\n",
    "                md_file.write(f\"    - **Number of Shares:** {details.get('number_of_shares', 'N/A')}\\n\")\n",
    "                md_file.write(f\"    - **Valuation:** {details.get('valuation', 'N/A')}\\n\")\n",
    "                md_file.write(f\"    - **Valuation Date:** {details.get('valuation_date', 'N/A')}\\n\")\n",
    "            md_file.write(f\"- **Age:** {insider.get('age', 'N/A')}\\n\")\n",
    "            md_file.write(f\"- **Industries:** {', '.join(literal_eval(insider.get('industries', [])))}\\n\")\n",
    "            md_file.write(f\"- **Summary:** {insider.get('summary', 'N/A')}\\n\")\n",
    "            md_file.write(\"- **Active Positions:**\\n\")\n",
    "            active_positions = dc.str_to_dict_expansion(insider.get('active_positions', '{}'))\n",
    "            for position, date in active_positions.items():\n",
    "                md_file.write(f\"    - {position}: {date}\\n\")\n",
    "            md_file.write(\"- **Former Positions:**\\n\")\n",
    "            former_positions = dc.str_to_dict_expansion(insider.get('former_positions', '{}'))\n",
    "            for position, date in former_positions.items():\n",
    "                md_file.write(f\"    - {position}: {date}\\n\")\n",
    "            md_file.write(\"- **Education:**\\n\")\n",
    "            trainings = dc.str_to_dict_expansion(insider.get('trainings', '{}'))\n",
    "            for training, details in trainings.items():\n",
    "                md_file.write(f\"    - {training}: {details}\\n\")\n",
    "            md_file.write(\"\\n---\\n\\n\")\n",
    "\n",
    "    print(f\"Markdown report saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/launchpad/workspace/services/Argus-fastapi/database/database_creator.py:677: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 677 of the file /Users/launchpad/workspace/services/Argus-fastapi/database/database_creator.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(soup)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Insider' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m example_link \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.marketscreener.com/quote/stock/BELSHIPS-ASA-1413091/news/Norne-Securities-lowers-target-price-for-Belships-to-NOK-28-30-reiterates-Buy-BN-48242972/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgenerate_markdown_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_link\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 105\u001b[0m, in \u001b[0;36mgenerate_markdown_report\u001b[0;34m(article_link, output_dir)\u001b[0m\n\u001b[1;32m    103\u001b[0m md_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## Linked Insiders\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m insider \u001b[38;5;129;01min\u001b[39;00m insiders:\n\u001b[0;32m--> 105\u001b[0m     md_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- **Name:** \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43minsider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m     md_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - **Current Position:** \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minsider\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent_position\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m     md_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - **Current Company:** \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minsider\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent_company\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/argus/lib/python3.11/site-packages/pydantic/main.py:828\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Insider' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "example_link = 'https://www.marketscreener.com/quote/stock/BELSHIPS-ASA-1413091/news/Norne-Securities-lowers-target-price-for-Belships-to-NOK-28-30-reiterates-Buy-BN-48242972/'\n",
    "generate_markdown_report(example_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to marketscreener_articles_2024-11-01.csv\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "endpoint_list = ['IPO', 'mergers-acquisitions', 'rumors']\n",
    "base_url = 'https://www.marketscreener.com/news/companies'\n",
    "csv_file_name = f\"marketscreener_articles_{datetime.now().strftime('%Y-%m-%d')}.csv\"\n",
    "dc.save_articles_to_csv(endpoint_list, base_url, csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test using archive.is to access paywalled news sites\n",
    "'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "def search_archive_is(original_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches archive.is for an existing archived URL of the given original URL.\n",
    "    \"\"\"\n",
    "    search_url = f\"https://archive.is/{original_url}\"\n",
    "    response = requests.get(search_url, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        # The search results usually have <a> tags pointing to the archived URL\n",
    "        result = soup.find_all('a', href=True)\n",
    "        if result:\n",
    "            return result\n",
    "    return None\n",
    "\n",
    "url = 'https://www.reuters.com/business/healthcare-pharmaceuticals/bicara-therapeutics-targets-265-mln-proceeds-upsized-us-ipo-2024-09-11/'\n",
    "test = search_archive_is(url)\n",
    "for item in test:\n",
    "    print(item['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
