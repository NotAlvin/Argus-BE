{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Imports for testing purposes\n",
    "'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from ast import literal_eval\n",
    "from datetime import datetime\n",
    "from utils.base_templates import NewsArticle, ArticleCollection, Company, Insider\n",
    "\n",
    "import database.database_creator as dc\n",
    "import database.database_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"database/insiders_2024-10-22.csv\"\n",
    "output_file = \"database/full_insiders_info_2024-10-22.csv\"\n",
    "\n",
    "dc.save_insiders_to_csv(input_file, output_file)\n",
    "dc.scrape_all_companies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'isin', 'ticker', 'industry', 'sector', 'profile', 'executives',\n",
       "       'link', 'country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('database/companies_info_2024-11-01.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown report saved to reports/Dabur_India_to_Buy_Majority_Stake_in_Sesa_Care_2024-10-30 16:00:00.md\n"
     ]
    }
   ],
   "source": [
    "def str_to_dict_expansion(dict_repr: any) -> dict:\n",
    "    dict_repr = str(dict_repr)\n",
    "    if dict_repr.strip() == '{}':\n",
    "        return {}\n",
    "    else:\n",
    "        try:\n",
    "            return ast.literal_eval(dict_repr.strip())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return {}\n",
    "\n",
    "def find_company_and_insiders_from_article(article_link: str):\n",
    "    # Step 1: Fetch the article from the Articles collection\n",
    "    articles_collection = du.get_db_collection(du.DB_NAME, 'Articles')\n",
    "    article = articles_collection.find_one({\"link\": article_link})\n",
    "    if not article:\n",
    "        print(\"Article not found in the database.\")\n",
    "        return None\n",
    "\n",
    "    # Step 2: Fetch the company linked to this article\n",
    "    company_link = 'https://www.marketscreener.com' + article.get('company_link', '')\n",
    "    companies_collection = du.get_db_collection(du.DB_NAME, 'Companies')\n",
    "    company = companies_collection.find_one({\"link\": company_link})\n",
    "    if not company:\n",
    "        print(\"Company not found in the database.\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: Get executive 'href' links to query the Insiders collection\n",
    "    executives = literal_eval(company.get('executives', '{}'))\n",
    "    insider_links = [\n",
    "        exec_info['href'] \n",
    "        for exec_category in executives.values() \n",
    "        for exec_info in exec_category.values()\n",
    "    ]\n",
    "\n",
    "    # Step 4: Fetch insiders based on the collected links\n",
    "    insiders_collection = du.get_db_collection(du.DB_NAME, 'Insiders')\n",
    "    insiders = list(insiders_collection.find({\"link\": {\"$in\": insider_links}}))\n",
    "\n",
    "    # Return the collected data as a dictionary\n",
    "    return {\n",
    "        \"article\": article,\n",
    "        \"company\": company,\n",
    "        \"insiders\": insiders\n",
    "    }\n",
    "\n",
    "def generate_markdown_report(article_link: str, output_dir: str = \"reports\"):\n",
    "    # Fetch article, company, and insiders data\n",
    "    data = find_company_and_insiders_from_article(article_link)\n",
    "    if data is None:\n",
    "        return\n",
    "\n",
    "    article = data[\"article\"]\n",
    "    company = data[\"company\"]\n",
    "    insiders = data[\"insiders\"]\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Generate the markdown file name using the article headline and date\n",
    "    headline = article.get('headline', 'unknown_headline').replace(' ', '_')\n",
    "    publication_date = article.get('publication_date', 'unknown_date')\n",
    "    file_name = f\"{headline}_{publication_date}.md\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # Write the markdown report\n",
    "    with open(file_path, 'w') as md_file:\n",
    "        md_file.write(f\"# {article.get('headline')}\\n\")\n",
    "        md_file.write(f\"**Publication Date:** {article.get('publication_date')}\\n\")\n",
    "        md_file.write(f\"**Category:** {article.get('category')}\\n\")\n",
    "        md_file.write(f\"**Source:** {article.get('source')}\\n\\n\")\n",
    "        md_file.write(\"## Article Content\\n\")\n",
    "        md_file.write(f\"{article.get('content')}\\n\\n\")\n",
    "        md_file.write(\"## Company Information\\n\")\n",
    "        md_file.write(f\"- **Name:** {company.get('name')}\\n\")\n",
    "        md_file.write(f\"- **ISIN:** {company.get('isin')}\\n\")\n",
    "        md_file.write(f\"- **Ticker:** {company.get('ticker')}\\n\")\n",
    "        md_file.write(f\"- **Industry:** {company.get('industry')}\\n\")\n",
    "        md_file.write(f\"- **Sector:** {company.get('sector')}\\n\")\n",
    "        md_file.write(f\"- **Country:** {company.get('country')}\\n\")\n",
    "        profile = company.get('profile', '').replace('-', '\\\\-')\n",
    "        md_file.write(f\"- **Profile:** {profile}\\n\\n\")\n",
    "        md_file.write(\"## Linked Insiders\\n\")\n",
    "        for insider in insiders:\n",
    "            md_file.write(f\"- **Name:** {insider.get('name')}\\n\")\n",
    "            md_file.write(f\"  - **Current Position:** {insider.get('current_position')}\\n\")\n",
    "            md_file.write(f\"  - **Current Company:** {insider.get('current_company')}\\n\")\n",
    "            md_file.write(f\"  - **Company URL:** {insider.get('company_url')}\\n\")\n",
    "            md_file.write(f\"  - **Net Worth:** {insider.get('net_worth', 'N/A')}\\n\")\n",
    "            known_holdings = str_to_dict_expansion(insider.get('known_holdings', '{}'))\n",
    "            md_file.write(f\"  - **Known Holdings:**\\n\")\n",
    "            for company, details in known_holdings.items():\n",
    "                md_file.write(f\"    - **{company}:**\\n\")\n",
    "                md_file.write(f\"      - **Link:** {details.get('link', 'N/A')}\\n\")\n",
    "                md_file.write(f\"      - **Date:** {details.get('date', 'N/A')}\\n\")\n",
    "                md_file.write(f\"      - **Number of Shares:** {details.get('number_of_shares', 'N/A')}\\n\")\n",
    "                md_file.write(f\"      - **Valuation:** {details.get('valuation', 'N/A')}\\n\")\n",
    "                md_file.write(f\"      - **Valuation Date:** {details.get('valuation_date', 'N/A')}\\n\")\n",
    "            md_file.write(f\"  - **Age:** {insider.get('age', 'N/A')}\\n\")\n",
    "            md_file.write(f\"  - **Industries:** {', '.join(literal_eval(insider.get('industries', [])))}\\n\")\n",
    "            md_file.write(f\"  - **Summary:** {insider.get('summary', 'N/A')}\\n\")\n",
    "            md_file.write(\"  - **Active Positions:**\\n\")\n",
    "            active_positions = str_to_dict_expansion(insider.get('active_positions', '{}'))\n",
    "            for position, date in active_positions.items():\n",
    "                md_file.write(f\"    - {position}: {date}\\n\")\n",
    "            md_file.write(\"  - **Former Positions:**\\n\")\n",
    "            former_positions = str_to_dict_expansion(insider.get('former_positions', '{}'))\n",
    "            for position, date in former_positions.items():\n",
    "                md_file.write(f\"    - {position}: {date}\\n\")\n",
    "            md_file.write(\"  - **Education:**\\n\")\n",
    "            trainings = str_to_dict_expansion(insider.get('trainings', '{}'))\n",
    "            for training, details in trainings.items():\n",
    "                md_file.write(f\"    - {training}: {details}\\n\")\n",
    "            md_file.write(\"\\n---\\n\\n\")\n",
    "\n",
    "    print(f\"Markdown report saved to {file_path}\")\n",
    "\n",
    "# Example usage\n",
    "df = pd.read_csv('database/marketscreener_articles_2024-11-01.csv')\n",
    "link = df.iloc[103].link\n",
    "generate_markdown_report(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "}\n",
    "\n",
    "def parse_time_string(time_str: str) -> datetime:\n",
    "    \"\"\"\n",
    "    Parses a time string into a datetime object with the current date or year.\n",
    "    \n",
    "    Args:\n",
    "        time_str (str): The time string to convert, e.g., '02:55am', 'Oct. 14', or '2024-10-20'.\n",
    "        \n",
    "    Returns:\n",
    "        datetime: A datetime object with the appropriate date and time.\n",
    "    \"\"\"\n",
    "    formats = [\n",
    "        ('%I:%M%p', lambda t: datetime.combine(datetime.now().date(), t.time())),\n",
    "        ('%b. %d', lambda d: datetime(datetime.now().year, d.month, d.day)),\n",
    "        ('%Y-%m-%d', lambda d: d)\n",
    "    ]\n",
    "    \n",
    "    for fmt, constructor in formats:\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(time_str, fmt)\n",
    "            return constructor(parsed_date)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    raise ValueError(f\"Time string '{time_str}' does not match expected formats.\")\n",
    "\n",
    "def fetch_html_content(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches and returns the HTML content of a given URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to fetch.\n",
    "        \n",
    "    Returns:\n",
    "        str: The HTML content of the page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        return urllib.parse.unquote(response.text)\n",
    "    except requests.RequestException as e:\n",
    "        raise RuntimeError(f\"Failed to fetch URL {url}: {e}\")\n",
    "\n",
    "def extract_article_text(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the article text from a BeautifulSoup object.\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object containing the HTML content.\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted article text.\n",
    "    \"\"\"\n",
    "    article_div = soup.find('div', class_='txt-s4 article-text')\n",
    "    if not article_div:\n",
    "        return 'Article text not found in the provided URL'\n",
    "    return article_div.get_text(separator='\\n', strip=True)\n",
    "\n",
    "def extract_marketscreener_article(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the article text from a MarketScreener article URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the MarketScreener article.\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted article text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        html_content = fetch_html_content(url)\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        return extract_article_text(soup)\n",
    "    except Exception as e:\n",
    "        return f'Error with extracting article text: {e}'\n",
    "\n",
    "def get_articles_from_marketscreener(url: str) -> ArticleCollection:\n",
    "    \"\"\"\n",
    "    Retrieves articles from a MarketScreener page.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the MarketScreener page.\n",
    "        \n",
    "    Returns:\n",
    "        ArticleCollection: A collection of articles extracted from the page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        html_content = fetch_html_content(url)\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        return ArticleCollection()\n",
    "\n",
    "    articles_list = []  # Initialize a list to store NewsArticle objects\n",
    "    category = \"MarketScreener \"  # Assuming the source is MarketScreener\n",
    "    extraction_date = datetime.now()  # Current date and time for extraction\n",
    "\n",
    "    tables = soup.find_all('table')\n",
    "    for table in tables:\n",
    "        for row in table.find_all('tr'):\n",
    "            headline_tag = row.find('a', class_='c txt-s1 txt-overflow-2 link link--no-underline my-5 my-m-0')\n",
    "            if not headline_tag:\n",
    "                continue\n",
    "\n",
    "            headline = headline_tag.text.strip()\n",
    "            link = 'https://www.marketscreener.com' + headline_tag.get('href')\n",
    "            # Find the company link\n",
    "            company_tag = row.find('a', class_='link link--blue c-flex align-top')\n",
    "            company_name = company_tag.get('title') if company_tag else None\n",
    "            company_link = company_tag.get('href') if company_tag else None\n",
    "\n",
    "            source_tag = row.find('span', class_='c-block p-5 badge badge--small txt-s1')\n",
    "            source = source_tag.get('title') if source_tag else source\n",
    "            time_tag = row.find('span', class_='js-date-relative txt-muted h-100')\n",
    "            publication_date = parse_time_string(time_tag.text.strip()) if time_tag else None\n",
    "\n",
    "            article = NewsArticle(\n",
    "                headline=headline,\n",
    "                content=extract_marketscreener_article(link),\n",
    "                link=link,\n",
    "                company_name=company_name,\n",
    "                company_link=company_link,\n",
    "                source=source,\n",
    "                publication_date=publication_date\n",
    "            )\n",
    "            articles_list.append(article)  # Append each NewsArticle to the list\n",
    "\n",
    "    # Create and return an ArticleCollection object with all fields filled\n",
    "    return ArticleCollection(\n",
    "        extraction_date=extraction_date,\n",
    "        articles=articles_list\n",
    "    )\n",
    "\n",
    "def save_articles_to_csv(endpoint_list, base_url, csv_file_name):\n",
    "    \"\"\"\n",
    "    Iterates through the endpoint list, retrieves article collections, and saves them to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        endpoint_list (list): List of endpoints to retrieve articles from.\n",
    "        base_url (str): The base URL for the MarketScreener news.\n",
    "        csv_file_name (str): The name of the CSV file to save the articles.\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "\n",
    "    for endpoint in endpoint_list:\n",
    "        url = f'{base_url}/{endpoint}/'\n",
    "        articles = get_articles_from_marketscreener(url)\n",
    "        \n",
    "        for article in articles.articles:\n",
    "            all_articles.append({\n",
    "                'category': endpoint,\n",
    "                'extraction_date': articles.extraction_date,\n",
    "                'headline': article.headline,\n",
    "                'content': article.content,\n",
    "                'company_name': article.company_name,\n",
    "                'company_link': article.company_link,\n",
    "                'link': article.link,\n",
    "                'source': article.source,\n",
    "                'publication_date': article.publication_date\n",
    "            })\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(all_articles)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(csv_file_name, index=False)\n",
    "    print(f\"Data saved to {csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to marketscreener_articles_2024-11-01.csv\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "endpoint_list = ['IPO', 'mergers-acquisitions', 'rumors']\n",
    "base_url = 'https://www.marketscreener.com/news/companies'\n",
    "csv_file_name = f\"marketscreener_articles_{datetime.now().strftime('%Y-%m-%d')}.csv\"\n",
    "save_articles_to_csv(endpoint_list, base_url, csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test using archive.is to access paywalled news sites\n",
    "'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "def search_archive_is(original_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches archive.is for an existing archived URL of the given original URL.\n",
    "    \"\"\"\n",
    "    search_url = f\"https://archive.is/{original_url}\"\n",
    "    response = requests.get(search_url, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        # The search results usually have <a> tags pointing to the archived URL\n",
    "        result = soup.find_all('a', href=True)\n",
    "        if result:\n",
    "            return result\n",
    "    return None\n",
    "\n",
    "url = 'https://www.reuters.com/business/healthcare-pharmaceuticals/bicara-therapeutics-targets-265-mln-proceeds-upsized-us-ipo-2024-09-11/'\n",
    "test = search_archive_is(url)\n",
    "for item in test:\n",
    "    print(item['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
